# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AV6U2ONwrH4AJWmM90I20nUMXEBqS875
"""

pip install streamlit

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
import streamlit as st

# Set random seed for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

# Define constants
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 10
NUM_CLASSES = 6
DATASET_PATH = "/content/drive/MyDrive/Project_Solar_Panel"  # Update with your dataset path
CLASSES = ["Clean", "Dusty", "Bird-Drop", "Electrical-Damage", "Physical-Damage", "Snow-Covered"]

# 1. Data Preprocessing and Augmentation
def load_and_preprocess_data():
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        validation_split=0.2  # 20% for validation
    )

    train_generator = train_datagen.flow_from_directory(
        DATASET_PATH,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        subset='training'
    )

    validation_generator = train_datagen.flow_from_directory(
        DATASET_PATH,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        subset='validation'
    )

    return train_generator, validation_generator

# 2. Exploratory Data Analysis (EDA)
def perform_eda(train_generator):
    # Class distribution
    class_counts = dict(zip(train_generator.class_indices.keys(),
                           np.bincount(train_generator.classes)))
    plt.figure(figsize=(10, 5))
    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))
    plt.title("Class Distribution in Training Set")
    plt.xlabel("Class")
    plt.ylabel("Number of Images")
    plt.xticks(rotation=45)
    plt.savefig("class_distribution.png")
    plt.close()

# 3. Build VGG16 Model
def build_model():
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze convolutional layers
    for layer in base_model.layers:
        layer.trainable = False

    # Add custom classification head
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(NUM_CLASSES, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# 4. Train Model
def train_model(model, train_generator, validation_generator):
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=validation_generator,
        verbose=1
    )
    return history

# 5. Evaluate Model
def evaluate_model(model, generator):
    predictions = model.predict(generator)
    y_pred = np.argmax(predictions, axis=1)
    y_true = generator.classes

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=CLASSES, yticklabels=CLASSES)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.savefig("confusion_matrix.png")
    plt.close()

    # Classification Report
    print("Classification Report:")
    print(classification_report(y_true, y_pred, target_names=CLASSES))

# 6. Streamlit App
def run_streamlit_app(model):
    st.title("SolarGuard: Solar Panel Defect Detection")
    st.write("Upload a solar panel image to classify its condition.")

    uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

    if uploaded_file is not None:
        # Load and preprocess image
        img = tf.keras.preprocessing.image.load_img(uploaded_file, target_size=IMG_SIZE)
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = img_array / 255.0  # Normalize
        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

        # Predict
        prediction = model.predict(img_array)
        predicted_class = CLASSES[np.argmax(prediction)]

        # Display results
        st.image(img, caption="Uploaded Image", use_column_width=True)
        st.write(f"Predicted Condition: **{predicted_class}**")
        st.write(f"Confidence: {np.max(prediction):.2%}")

# Main function
def main():
    # Load data
    train_generator, validation_generator = load_and_preprocess_data()

    # Perform EDA
    perform_eda(train_generator)

    # Build and train model
    model = build_model()
    history = train_model(model, train_generator, validation_generator)

    # Evaluate model
    evaluate_model(model, validation_generator)

    # Plot training history
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig("training_history.png")
    plt.close()

    # Run Streamlit app (comment out if running in non-Streamlit environment)
    run_streamlit_app(model)

if __name__ == "__main__":
    main()

!pip install streamlit
!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("Solar Panel Condition Classification")
# st.write("Upload an image and get classification results.")
# 
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "png"])
# if uploaded_file is not None:
#     st.image(uploaded_file, caption="Uploaded Image", use_column_width=True)
#     st.write("Classifying...")
#     # You can load your model and predict here
#

from pyngrok import ngrok
ngrok.set_auth_token("2xdxyfWBIvI2QHqFqYeD2oGv28F_2YHzp1LbvPfFHTWftDeG7")

!pip install streamlit pyngrok --quiet

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("Solar Panel Condition Classification")
# st.write("Upload an image and get classification results.")
# 
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "png"])
# if uploaded_file is not None:
#     st.image(uploaded_file, caption="Uploaded Image", use_column_width=True)
#     st.write("Classifying...")
#     # Add model loading and prediction here
#

from pyngrok import ngrok

# ðŸ”‘ Use your ngrok authtoken (get it from https://dashboard.ngrok.com/get-started/your-authtoken)
ngrok.set_auth_token("2xdxyfWBIvI2QHqFqYeD2oGv28F_2YHzp1LbvPfFHTWftDeG7")

# ðŸ—ï¸ Start the tunnel with correct HTTP configuration
public_url = ngrok.connect(addr="http://localhost:8501", proto="http")
print(f"Your Streamlit app is live at: {public_url}")

# Start Streamlit app
!streamlit run app.py &>/dev/null &

"""https://5905-35-224-252-253.ngrok-free.app/"""

